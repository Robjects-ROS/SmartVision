{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Software Checklist \n",
    "\n",
    "In order to run the software, it is preferable to have all the steps in the hardware configuration checklist completed; however, it is not mandatory. You can manually install the software on any linux based system with a little bit of effort.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the detection accuracy of Yolov4-tiny has declined, but in terms of timeconsumption, Yolov4-tiny has obvious advantages:Yolov4-tiny detection takes only 2.6milliseconds, while Yolov4 detection takes 27 milliseconds, which is more than 10 times faster!\n",
    "\n",
    "# Environmental requirements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorflow-gpu==2.2.0\n",
    "lxml\n",
    "matplotlib\n",
    "pandas\n",
    "Pillow\n",
    "scikit-learn\n",
    "seaborn\n",
    "tqdm\n",
    "imgaug\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model training process\n",
    "(Folder structuregarbage_data: store data set\n",
    "garbage_data/image: target source file\n",
    "garbage_data/JPEGImages: data set ictures \n",
    "(as many as possible)\n",
    "ground picture (as many as possible) garbage_data/train.txt: label file corresponding to the data set image garbage_data/GetData.py: get data set code\n",
    "\n",
    "font: store font package\n",
    "\n",
    "img: store test images\n",
    "\n",
    "logs: stores test logs and the final training model last1.h5.\n",
    "\n",
    "model_data: stores pre-trained models (weight files), custom label files (corresponding to target source files), and yolo model parameter anchors.\n",
    "\n",
    "nets and utils: some library files of yolo\n",
    "\n",
    "In the YOLO-v2 version, the concept of anchor box was introduced, which greatly increased the performance of target detection. The essence of anchor is the reverse of the SPP (spatial pyramid pooling) idea.What SPP itself does is to resize inputs of different sizes into outputs of the same size, so the reverse of SPP is to reverse the output of the same size to get inputs of different sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2)Training steps\n",
    "\n",
    "Training code source network, linkï¼šhttps://github.com/bubbliiiing/yolov4-tiny-tf2\n",
    "\n",
    " Make data sets\n",
    "\n",
    "The names of the pictures and label files must correspond. The label format in the train.txt file is as follows:\n",
    "\n",
    "./garbage_data/JPEGImages/0.jpg  113,163,293,298,9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Picture path    y, x, y + w, x + h ,label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a data set, one method is to take some photos first, use an annotation tool to annotate the targets on each photo, create a new train.txt file in the garbage_data folder, and write the target information.\n",
    "\n",
    "Another method is to put background images (as many as possible) in the garbage_data/texture folder, modify the GetData.py code as needed, and execute GetData.py to generate a data set (as many as possible).\n",
    "\n",
    " Add weight file\n",
    "\n",
    "You can search and download the latest weight file on Baidu. There are good weight files yolov4_tiny_weights_coco.h5 and yolov4_tiny_weights_voc.h5 under the garbage_data file.\n",
    "\n",
    " Make your own classes--->garbage.txt\n",
    "\n",
    "Note that it is best not to use Chinese tags and there should be no spaces in the folder!\n",
    "\n",
    "Zip_top_can Old_school_bag Newspaper Book Toilet_paper \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify train.py file\n",
    "\n",
    "Modify according to your own needs by referring to the comments.\n",
    "\n",
    "Label location\n",
    "\n",
    "annotation_path = 'garbage_data/train.txt'\n",
    "\n",
    "Get the location of classes and anchor\n",
    "\n",
    "classes_path = 'model_data/garbage.txt'\n",
    "\n",
    "anchors_path = 'model_data/yolo_anchors.txt'\n",
    "\n",
    "Location of pre-trained model\n",
    "\n",
    "weights_path = 'model_data/yolov4_tiny_weights_coco.h5'\n",
    "\n",
    "Get classes and anchor\n",
    "\n",
    "class_names = get_classes(classes_path)\n",
    "\n",
    "anchors = get_anchors(anchors_path)\n",
    "\n",
    "How many categories are there in total?\n",
    "\n",
    "num_classes = len(class_names)\n",
    "\n",
    "num_anchors = len(anchors)\n",
    "\n",
    "The location where the trained model is saved log_dir = 'logs/'\n",
    "\n",
    "Enter the image size. If the video memory is large, 608x608 can be used. input_shape = (416,416)\n",
    "\n",
    "Initial epoch value\n",
    "\n",
    "Init_epoch = 0\n",
    "\n",
    "Freeze the epoch value of training\n",
    "\n",
    "Freeze_epoch = 50\n",
    "\n",
    "The size of Batch_size indicates how much data is fed each time. If there is OOM or insufficient video memory, please adjust it smaller. batch_size = 16\n",
    "\n",
    "Maximum learning rate\n",
    "\n",
    "learning_rate_base = 1e-3\n",
    "\n",
    "Total epoch value\n",
    "\n",
    "Epoch = 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Start training\n",
    "\n",
    "According to the above process, after the operation is completed, directly run the train.py file for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) Custom model detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " Modify yolov.py file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class YOLO(object):\n",
    "\n",
    "    _defaults = {\n",
    "\n",
    "      \"model_path\": 'model_data/garbage.h5',\n",
    "\n",
    "      \"anchors_path\": 'model_data/yolo_anchors.txt',       \"classes_path\": 'model_data/garbage.txt',\n",
    "\n",
    "      \"score\" : 0.5,\n",
    "\n",
    "      \"iou\" : 0.3,\n",
    "\n",
    "      \"eager\" : False,\n",
    "\n",
    "# The default is 416x416 (image size)\n",
    "\n",
    "      \"model_image_size\" : (416, 416)\n",
    "\n",
    "   }\n",
    "\n",
    "self.font_path = 'font/Block_Simplified.TTF'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "- `model_path`: used for detection and trained model path (global path is required in ROS environment).\n",
    "- `anchors_path`: yolo's model parameter anchors path (the global path is required in ROS environment).\n",
    "- `classes_path`: Custom label file path (global path is required in ROS environment).\n",
    "- `self.font_path`: font package path (global path is required in ROS environment).\n",
    "\n",
    "l Execute py file detection\n",
    "\n",
    "predict_img.py: Image detection.\n",
    "\n",
    "predict_video.py: Video detection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.6.15",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
